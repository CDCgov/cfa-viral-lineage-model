data:
  # Where are the processed datasets for modeling and evaluation stored?
  save_file:
    model: data/2022-early-omicron/model.parquet
    eval: data/2022-early-omicron/eval.parquet

  # What is the forecast date?
  # No sequences collected or reported after this date are included in the
  # modeling dataset.
  forecast_date:
    year: 2022
    month: 2
    day: 14

  # The modeling dataset will contain sequences collected and reported within
  # `[lower, 0]`, where 0 is the above `forecast_date`.
  # The evaluation dataset will contain sequences collected within `[lower, upper]`
  # and reported at any point before the date the data was accessed.
  horizon:
    lower: -90
    upper: 14

  # Which lineages should be included?
  # All others will be aggregated into "other".
  lineages: ["21I", "21J", "21K", "21L"]

  # UShER recoding will use sequences from forecast_date + datetime.timedelta(days=usher_lag)
  usher_lag: 168

forecasting:
  # Where (directory) should model output and visualizations be stored?
  save_dir: out/forecasts/2022-early-omicron/

  # Which models should be fit?
  models:
    - BaselineModel
    - IndependentDivisionsModel
    - HierarchicalDivisionsModel

  # MCMC configuration
  mcmc:
    warmup: 3000
    samples: 3000
    thinning: 12
    chains: 4
    convergence:
      # one of: "all" or "failing" to write a parquet of ESS and PSRF
      # for all parameters or only those failing
      report_mode: "failing"
      # Should diagnostic plots for all reported parameters be made?
      plot: False
      # ESS below this are considered bad
      ess_cutoff: 500
      # PSRF above this are considered bad
      psrf_cutoff: 1.01

  # Settings for HHS region, population-weighted aggregate forecasts
  survey:
    # Should the HHS region forecasts be made
    make: False
    # Should the HHS region forecasts be plotted (they are not evaluable!)
    plot: False
    # Where to get population size data from, path to a CSV
    pop_sizes: "pop_sizes_nst_est_2023.csv"

evaluation:
  # Where (directory) should model scores and visualizations be stored?
  save_dir: out/eval/2022-early-omicron

  # Each element of `filters` is a dictionary specifying filters for evaluation.
  # Keys are columns in a ForecastFrame or CountsFrame, which will be filtered
  # to only values in the dictionary values for that key (or null for no filter for that column).
  # At least one filter must be specified, it may be null (to evaluate everything).
  filters:
    # Evaluate everything
    everything: null
    # Evaluate all lineages and divisions but only for the forecast date and period
    forecast_days:
      fd_offset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]

  # How should forecasts be evaluated?
  metrics:
    - ProportionsEvaluator
    - CountsEvaluator:
      - count_sampler: multinomial
      - seed: 42
